import pandas as pd
import numpy as np
import logging
from datetime import datetime, timedelta
import sys

from config import config
from data_loader import DataLoader
from sentiment_analyzer import SentimentAnalyzer
from feature_engine import FeatureEngine
from model import PredictionModel
from gpu_manager import setup_gpu

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class InvestmentPredictionSystem:
    """Sistema completo de predi√ß√£o de investimentos"""
    
    def __init__(self):
        self.data_loader = DataLoader()
        self.sentiment_analyzer = SentimentAnalyzer()
        self.feature_engine = FeatureEngine()
        self.model = PredictionModel()
        self.final_df = None
    
    def run_pipeline(self, skip_sentiment: bool = False) -> pd.DataFrame:
        """
        Executa pipeline completo do sistema
        
        Args:
            skip_sentiment: Se True, pula an√°lise de sentimento (√∫til para testes r√°pidos)
        
        Returns:
            DataFrame final com todas as features
        """
        logger.info("=" * 60)
        logger.info("üöÄ SISTEMA DE PREDI√á√ÉO DE INVESTIMENTOS - PETR4")
        logger.info("=" * 60)
        
        # Configurar GPU
        logger.info("\nüéÆ CONFIGURA√á√ÉO DE GPU")
        logger.info("-" * 60)
        gpu_manager = setup_gpu(
            gpu_id=config.GPU_ID,
            memory_growth=config.GPU_MEMORY_GROWTH,
            memory_limit_mb=config.GPU_MEMORY_LIMIT_MB
        )
        gpu_manager.print_gpu_info()
        logger.info(f"Dispositivo selecionado: {gpu_manager.get_current_device()}")
        
        # Validar configura√ß√£o
        try:
            config.validate()
        except Exception as e:
            logger.error(f"‚ùå Erro na valida√ß√£o: {e}")
            sys.exit(1)
        
        # 1. Carregar dados
        logger.info("\nüìÇ ETAPA 1: Carregamento de Dados")
        logger.info("-" * 60)
        df = self.data_loader.merge_datasets()
        
        # 2. An√°lise de sentimento
        if not skip_sentiment:
            logger.info("\nüß† ETAPA 2: An√°lise de Sentimento (Gemini API)")
            logger.info("-" * 60)
            df = self._add_sentiment_analysis(df)
        else:
            logger.info("\n‚ö†Ô∏è  Pulando an√°lise de sentimento")
            df["sentiment"] = 0
            df["sentiment_confidence"] = 0.5
        
        # 3. Feature engineering
        logger.info("\n‚öôÔ∏è  ETAPA 3: Feature Engineering")
        logger.info("-" * 60)
        df = self.feature_engine.create_all_features(df)
        
        # 4. Adicionar target
        df = self.feature_engine.add_target_variable(df)
        
        # 5. Salvar dataset processado
        logger.info("\nüíæ ETAPA 4: Salvamento de Dados")
        logger.info("-" * 60)
        df.to_csv(config.OUTPUT_FILE, index=False, encoding="utf-8-sig")
        logger.info(f"‚úÖ Dataset salvo: {config.OUTPUT_FILE}")
        
        self.final_df = df
        
        logger.info("\n" + "=" * 60)
        logger.info("‚úÖ PIPELINE CONCLU√çDO COM SUCESSO")
        logger.info("=" * 60)
        logger.info(f"üìä Total de registros: {len(df)}")
        logger.info(f"üìä Total de features: {len(df.columns)}")
        logger.info("=" * 60)
        
        return df
    
    def _add_sentiment_analysis(self, df: pd.DataFrame) -> pd.DataFrame:
        """Adiciona an√°lise de sentimento ao DataFrame"""
        texts = df["text"].tolist()
        
        # Analisar sentimento em batch
        results = self.sentiment_analyzer.batch_analyze(texts)
        
        # Adicionar ao DataFrame
        sentiments = []
        confidences = []
        
        for i in range(len(df)):
            if i in results:
                sent, conf = results[i]
                sentiments.append(sent)
                confidences.append(conf)
            else:
                sentiments.append(0)
                confidences.append(0.5)
        
        df["sentiment"] = sentiments
        df["sentiment_confidence"] = confidences
        
        logger.info(f"‚úÖ Sentimentos analisados: {len(results)}")
        logger.info(f"   Positivos: {sum(1 for s in sentiments if s == 1)}")
        logger.info(f"   Neutros: {sum(1 for s in sentiments if s == 0)}")
        logger.info(f"   Negativos: {sum(1 for s in sentiments if s == -1)}")
        
        return df
    
    def train_model(self, df: pd.DataFrame = None) -> dict:
        """
        Treina modelo de predi√ß√£o
        
        Args:
            df: DataFrame processado (usa self.final_df se None)
            
        Returns:
            Dicion√°rio com m√©tricas de avalia√ß√£o
        """
        logger.info("\n" + "=" * 60)
        logger.info("üéØ TREINAMENTO DO MODELO")
        logger.info("=" * 60)
        
        if df is None:
            if self.final_df is None:
                raise ValueError("Execute run_pipeline() primeiro")
            df = self.final_df
        
        # Validar dados m√≠nimos
        min_samples = config.SEQUENCE_LENGTH * 3  # M√≠nimo 3x o sequence length
        if len(df) < min_samples:
            logger.error(f"‚ùå Dataset muito pequeno: {len(df)} amostras")
            logger.error(f"   M√≠nimo necess√°rio: {min_samples} amostras")
            logger.error(f"   Adicione mais dados hist√≥ricos ou reduza SEQUENCE_LENGTH em config.py")
            raise ValueError(f"Dataset insuficiente: {len(df)} < {min_samples}")
        
        # Split temporal
        split_idx = int(len(df) * config.TRAIN_TEST_SPLIT)
        
        # Garantir m√≠nimo de amostras para teste
        min_test = config.SEQUENCE_LENGTH + 2
        if len(df) - split_idx < min_test:
            split_idx = len(df) - min_test
            logger.warning(f"‚ö†Ô∏è  Ajustado split para garantir teste m√≠nimo")
        
        train_df = df.iloc[:split_idx].copy()
        test_df = df.iloc[split_idx:].copy()
        
        logger.info(f"üìä Split: {len(train_df)} treino / {len(test_df)} teste")
        
        # Validar tamanho ap√≥s split
        if len(test_df) <= config.SEQUENCE_LENGTH:
            logger.error(f"‚ùå Dados de teste insuficientes: {len(test_df)}")
            raise ValueError("Dataset de teste muito pequeno ap√≥s split")
        
        # Preparar dados
        logger.info("\n‚öôÔ∏è  Preparando dados...")
        X_train, y_train = self.model.prepare_data(train_df)
        
        if len(X_train) == 0:
            logger.error("‚ùå Nenhuma sequ√™ncia de treino criada")
            raise ValueError("Dataset de treino insuficiente para criar sequ√™ncias")
        
        # Preparar teste (usar mesmo scaler)
        X_test = test_df[self.model.feature_columns].values
        y_test = test_df["target_price"].values.reshape(-1, 1)
        
        X_test_scaled = self.model.scaler_features.transform(X_test)
        y_test_scaled = self.model.scaler_target.transform(y_test)
        
        X_test_seq, y_test_seq = self.model.create_sequences(X_test_scaled, y_test_scaled)
        
        if len(X_test_seq) == 0:
            logger.error("‚ùå Nenhuma sequ√™ncia de teste criada")
            raise ValueError("Dataset de teste insuficiente para criar sequ√™ncias")
        
        # Criar valida√ß√£o (√∫ltimos X% do treino)
        val_split_ratio = config.VALIDATION_SPLIT
        val_split = int(len(X_train) * (1 - val_split_ratio))
        
        if val_split < 1:
            # Dataset muito pequeno, treinar sem valida√ß√£o
            logger.warning("‚ö†Ô∏è  Dataset pequeno: treinando sem valida√ß√£o")
            X_val = None
            y_val = None
        else:
            X_val = X_train[val_split:]
            y_val = y_train[val_split:]
            X_train = X_train[:val_split]
            y_train = y_train[:val_split]
        
        logger.info(f"‚úÖ Treino: {len(X_train)} | Valida√ß√£o: {len(X_val) if X_val is not None else 0} | Teste: {len(X_test_seq)}")
        
        # Treinar
        logger.info("\nüöÇ Iniciando treinamento...\n")
        history = self.model.train(X_train, y_train, X_val, y_val)
        
        # Avaliar
        logger.info("\nüìä Avaliando modelo...")
        metrics = self.model.evaluate(X_test_seq, y_test_seq)
        
        # Salvar modelo
        self.model.save_model()
        
        logger.info("\n" + "=" * 60)
        logger.info("‚úÖ TREINAMENTO CONCLU√çDO")
        logger.info("=" * 60)
        
        return metrics
    
    def predict_next_day(self, df: pd.DataFrame = None) -> dict:
        """
        Faz predi√ß√£o para o pr√≥ximo dia de negocia√ß√£o
        
        Args:
            df: DataFrame processado (usa self.final_df se None)
            
        Returns:
            Dicion√°rio com predi√ß√£o e informa√ß√µes
        """
        logger.info("\n" + "=" * 60)
        logger.info("üîÆ PREDI√á√ÉO PARA PR√ìXIMO DIA")
        logger.info("=" * 60)
        
        if df is None:
            if self.final_df is None:
                raise ValueError("Execute run_pipeline() primeiro")
            df = self.final_df
        
        # Preparar √∫ltimos N dias
        last_n_days = df.tail(config.SEQUENCE_LENGTH).copy()
        
        # Extrair features
        X = last_n_days[self.model.feature_columns].values
        X_scaled = self.model.scaler_features.transform(X)
        X_seq = X_scaled.reshape(1, config.SEQUENCE_LENGTH, -1)
        
        # Predizer
        prediction_scaled = self.model.model.predict(X_seq, verbose=0)
        prediction = self.model.scaler_target.inverse_transform(prediction_scaled)[0][0]
        
        # Informa√ß√µes contextuais
        last_price = df["close"].iloc[-1]
        last_date = df["date"].iloc[-1]
        next_date = last_date + timedelta(days=1)
        
        change = prediction - last_price
        change_percent = (change / last_price) * 100
        direction = "ALTA ‚¨ÜÔ∏è" if change > 0 else "BAIXA ‚¨áÔ∏è"
        
        result = {
            "last_date": last_date.strftime("%Y-%m-%d"),
            "next_date": next_date.strftime("%Y-%m-%d"),
            "last_price": last_price,
            "predicted_price": prediction,
            "change": change,
            "change_percent": change_percent,
            "direction": direction
        }
        
        # Exibir resultado
        logger.info(f"\nüìÖ √öltima data: {result['last_date']}")
        logger.info(f"üí∞ √öltimo pre√ßo: R$ {result['last_price']:.2f}")
        logger.info(f"\nüîÆ Predi√ß√£o para {result['next_date']}:")
        logger.info(f"   Pre√ßo: R$ {result['predicted_price']:.2f}")
        logger.info(f"   Varia√ß√£o: R$ {result['change']:.2f} ({result['change_percent']:+.2f}%)")
        logger.info(f"   Tend√™ncia: {result['direction']}")
        logger.info("=" * 60)
        
        return result


def main():
    """Fun√ß√£o principal"""
    try:
        # Criar sistema
        system = InvestmentPredictionSystem()
        
        # Executar pipeline completo
        df = system.run_pipeline(skip_sentiment=False)
        
        # Treinar modelo
        metrics = system.train_model(df)
        
        # Fazer predi√ß√£o
        prediction = system.predict_next_day(df)
        
        logger.info("\n‚úÖ Sistema executado com sucesso!")
        
    except KeyboardInterrupt:
        logger.info("\n‚ö†Ô∏è  Execu√ß√£o interrompida pelo usu√°rio")
        sys.exit(0)
    
    except Exception as e:
        logger.error(f"\n‚ùå Erro fatal: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()
